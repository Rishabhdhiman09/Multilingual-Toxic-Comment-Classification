{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport string\nimport re\nimport nltk\nimport spacy\nimport random\nimport missingno as msno\nimport tensorflow as tf\nfrom transformers import TFAutoModel, AutoTokenizer\nimport os\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import shuffle\nimport tensorflow.keras.backend as K\n%matplotlib inline","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tr = pd.read_csv(\"../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr-cleaned.csv\")\ntrain_tr =shuffle( pd.concat([\n    train_tr[[\"comment_text\", \"toxic\"]].query(\"toxic == 1\"),\n    train_tr[[\"comment_text\", \"toxic\"]].query(\"toxic == 0\").sample(30000, random_state = 1)\n]))\n\ntrain_ru = pd.read_csv(\"../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru-cleaned.csv\")\ntrain_ru = shuffle(pd.concat([\n    train_ru[[\"comment_text\", \"toxic\"]].query(\"toxic == 1\"),\n    train_ru[[\"comment_text\", \"toxic\"]].query(\"toxic == 0\").sample(30000, random_state = 1)\n]))\n\ntrain_it = pd.read_csv(\"../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it-cleaned.csv\")\ntrain_it = shuffle(pd.concat([\n    train_it[[\"comment_text\", \"toxic\"]].query(\"toxic == 1\"),\n    train_it[[\"comment_text\", \"toxic\"]].query(\"toxic == 0\").sample(30000, random_state = 1)\n]))\n\ntrain_fr = pd.read_csv(\"../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr-cleaned.csv\")\ntrain_fr = shuffle(pd.concat([\n    train_fr[[\"comment_text\", \"toxic\"]].query(\"toxic == 1\"),\n    train_fr[[\"comment_text\", \"toxic\"]].query(\"toxic == 0\").sample(30000, random_state = 1)\n]))\n\ntrain_pt = pd.read_csv(\"../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt-cleaned.csv\")\ntrain_pt = shuffle(pd.concat([\n    train_pt[[\"comment_text\", \"toxic\"]].query(\"toxic == 1\"),\n    train_pt[[\"comment_text\", \"toxic\"]].query(\"toxic == 0\").sample(30000, random_state = 1)\n]))\n\ntrain_es = pd.read_csv(\"../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es-cleaned.csv\")\ntrain_es = shuffle(pd.concat([\n    train_es[[\"comment_text\", \"toxic\"]].query(\"toxic == 1\"),\n    train_es[[\"comment_text\", \"toxic\"]].query(\"toxic == 0\").sample(30000, random_state = 1)\n]))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\ntest = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\nsub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv') ","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_ru = shuffle(pd.concat([train_ru.query(\"toxic == 1\").sample(1250),\n                     pd.read_csv(\"../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr-cleaned.csv\")[[\"comment_text\", \"toxic\"]]\\\n                      .query(\"toxic == 0\").sample(1250)]))\n\nvalid_fr = shuffle(pd.concat([train_fr.query(\"toxic == 1\").sample(1250),\n                     pd.read_csv(\"../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr-cleaned.csv\")[[\"comment_text\", \"toxic\"]]\\\n                      .query(\"toxic == 0\").sample(1250)]))\n\nvalid_pt = shuffle(pd.concat([train_pt.query(\"toxic == 1\").sample(1250),\n                     pd.read_csv(\"../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt-cleaned.csv\")[[\"comment_text\", \"toxic\"]]\\\n                      .query(\"toxic == 0\").sample(1250)]))\n\nvalid_it = valid[[\"comment_text\", \"lang\", \"toxic\"]].query(\"lang == 'it'\")\nvalid_es = valid[[\"comment_text\", \"lang\", \"toxic\"]].query(\"lang == 'es'\")\nvalid_tr = valid[[\"comment_text\", \"lang\", \"toxic\"]].query(\"lang == 'tr'\")","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feeding all six languages to xlm roberta","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = shuffle(pd.concat([\n    train_ru,\n    train_es,\n    train_fr,\n    train_it,\n    train_pt,\n    train_tr\n]))\n\nvalid = shuffle(pd.concat([\n    valid_ru,\n    valid_es,\n    valid_fr,\n    valid_it,\n    valid_pt,\n    valid_tr\n]))","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([\n    train,\n    valid\n]).sample(120000, random_state = 1)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-xlm-roberta-large\")","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 192","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(data, max_seq_length = MAX_LEN, tokenizer = tokenizer):    \n    ids = []\n    masks = []\n    segment = []\n    for i in tqdm(range(len(data))):\n        \n        tokens = tokenizer.tokenize(data[i])\n        if len(tokens) > max_seq_length - 2:\n            tokens = tokens[ : max_seq_length - 2]\n\n        # Converting tokens to ids\n        input_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"])\n\n        # Input mask\n        input_masks = [1] * len(input_ids)\n\n        # padding upto max length\n        padding = max_seq_length - len(input_ids)\n        input_ids.extend([0] * padding)\n        input_masks.extend([0] * padding)\n        segment_ids =[0]* max_seq_length\n        \n        \n        ids.append(input_ids)\n        masks.append(input_masks)\n        segment.append(segment_ids)\n    \n    return (np.array(ids), np.array(masks), np.array(segment))","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ids, train_masks, train_segment =  preprocess(train[\"comment_text\"].values)","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=120000.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fe4448bec39424591d3b7a35e5fb7ee"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids, test_masks, test_segment =  preprocess(test[\"content\"].values)","execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=63812.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f8c88d6b55c4a448435a50da709e222"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train[\"toxic\"].values","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":24,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16 * strategy.num_replicas_in_sync","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(roberta_layer, max_len = MAX_LEN):\n    \n        input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n        input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n        segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n        pooled_output, sequence_output = roberta_layer([input_word_ids, input_mask, segment_ids])\n\n        # There are two outputs: a pooled_output of shape [batch_size, 768] with representations for \n        # the entire input sequences and a sequence_output of shape [batch_size, max_seq_length, 768] \n        # with representations for each input token (in context)\n\n\n        x = pooled_output\n#         x = tf.keras.layers.Flatten()(x)\n#         x = tf.keras.layers.Dense(128, activation = \"relu\")(x)\n#         x = tf.keras.layers.Dense(1, activation = \"sigmoid\")(x)\n        x1 = tf.keras.layers.Dropout(0.1)(x) \n        x1 = tf.keras.layers.Conv1D(128,2, padding = \"same\")(x1)\n        x1 = tf.keras.layers.ReLU()(x1)\n        x1 = tf.keras.layers.Conv1D(16,2, padding = \"same\")(x1)\n        x1 = tf.keras.layers.ReLU()(x1)\n        x1 = tf.keras.layers.Dense(1)(x1)\n        x1 = tf.keras.layers.Flatten()(x1)\n        x = tf.keras.layers.Dense(1, activation = \"sigmoid\")(x1)\n\n\n        model = tf.keras.Model(inputs = [input_word_ids, input_mask, segment_ids], outputs = x)\n        return model","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    roberta_layer = TFAutoModel.from_pretrained(\"jplu/tf-xlm-roberta-large\", trainable = True)\n    model = model(roberta_layer)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()","execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3271420488.0, style=ProgressStyle(descrâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7a34f414c42484e98d09aa7a156116c"}},"metadata":{}},{"output_type":"stream","text":"\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 192)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 192)]        0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 192)]        0                                            \n__________________________________________________________________________________________________\ntf_roberta_model (TFRobertaMode ((None, 192, 1024),  559890432   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n__________________________________________________________________________________________________\ndropout_74 (Dropout)            (None, 192, 1024)    0           tf_roberta_model[0][0]           \n__________________________________________________________________________________________________\nconv1d (Conv1D)                 (None, 192, 128)     262272      dropout_74[0][0]                 \n__________________________________________________________________________________________________\nre_lu (ReLU)                    (None, 192, 128)     0           conv1d[0][0]                     \n__________________________________________________________________________________________________\nconv1d_1 (Conv1D)               (None, 192, 16)      4112        re_lu[0][0]                      \n__________________________________________________________________________________________________\nre_lu_1 (ReLU)                  (None, 192, 16)      0           conv1d_1[0][0]                   \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 192, 1)       17          re_lu_1[0][0]                    \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 192)          0           dense[0][0]                      \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1)            193         flatten[0][0]                    \n==================================================================================================\nTotal params: 560,157,026\nTrainable params: 560,157,026\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=3, shuffle = True)\nskf.get_n_splits(train_ids, y_train)\n\ni = 1\npreds = []\nfor train_index, test_index in skf.split(train_ids, y_train):\n    print(\"\\n\")\n    print(\"#\" * 20)\n    print(f\"FOLD No {i}\")\n    print(\"#\" * 20)\n    \n    \n    tr_ids = train_ids[train_index]\n    tr_masks = train_masks[train_index]\n    tr_segment = train_segment[train_index]\n    \n    vd_ids = train_ids[test_index]\n    vd_masks = train_masks[test_index]\n    vd_segment = train_segment[test_index]\n    \n    y_tr = y_train[train_index]\n    y_vd = y_train[test_index]\n    \n    \n    history = model.fit(\n    (tr_ids, tr_masks, tr_segment), y_tr,\n    epochs=2,\n    batch_size=BATCH_SIZE,\n    validation_data = ((vd_ids, vd_masks, vd_segment), y_vd),\n    steps_per_epoch = len(tr_ids)//BATCH_SIZE)\n\n    predictions = model.predict((test_ids, test_masks, test_segment))\n    preds.append(predictions)\n    \n    i += 1\n    K.clear_session()\n","execution_count":28,"outputs":[{"output_type":"stream","text":"\n\n####################\nFOLD No 1\n####################\nEpoch 1/2\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n  num_elements)\n","name":"stderr"},{"output_type":"stream","text":"625/625 [==============================] - 302s 483ms/step - loss: 0.2995 - accuracy: 0.8652 - val_loss: 0.2038 - val_accuracy: 0.9174\nEpoch 2/2\n625/625 [==============================] - 268s 429ms/step - loss: 0.2059 - accuracy: 0.9153 - val_loss: 0.1918 - val_accuracy: 0.9236\n\n\n####################\nFOLD No 2\n####################\nEpoch 1/2\n625/625 [==============================] - 269s 430ms/step - loss: 0.1894 - accuracy: 0.9232 - val_loss: 0.1587 - val_accuracy: 0.9365\nEpoch 2/2\n625/625 [==============================] - 269s 430ms/step - loss: 0.1618 - accuracy: 0.9348 - val_loss: 0.1480 - val_accuracy: 0.9409\n\n\n####################\nFOLD No 3\n####################\nEpoch 1/2\n625/625 [==============================] - 270s 433ms/step - loss: 0.1502 - accuracy: 0.9416 - val_loss: 0.1006 - val_accuracy: 0.9634\nEpoch 2/2\n625/625 [==============================] - 271s 433ms/step - loss: 0.1188 - accuracy: 0.9545 - val_loss: 0.1016 - val_accuracy: 0.9623\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = (0.2 * preds[0] + 0.3 * preds[1] + 0.5 * preds[2])","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[\"toxic\"] = predictions\nsub.set_index(\"id\", inplace = True)\nsub.to_csv(\"submission.csv\")","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}